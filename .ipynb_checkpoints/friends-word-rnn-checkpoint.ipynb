{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./data/friends/all_scripts.txt\") as scripts_fileobj:\n",
    "    all_scripts = scripts_fileobj.read().strip().lower().decode('utf8').encode('ascii', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    dialogues = []\n",
    "    dialogue = []\n",
    "    for line in lines:\n",
    "        if \":\" in line:\n",
    "            dialogues.append(\" \".join(dialogue))\n",
    "            dialogue = []\n",
    "            dialogue.append(line)\n",
    "        else:\n",
    "            dialogue.append(line)\n",
    "    \n",
    "    text = \"\\n\".join(dialogues)\n",
    "    punctuations = set(re.findall(r\"[^a-zA-Z0-9 ]\",text))\n",
    "    for punctuation in punctuations:\n",
    "        if punctuation == \"\\n\":\n",
    "            text = text.replace(punctuation,\" NEWLINE \")\n",
    "        else:\n",
    "            text = text.replace(punctuation,\" \"+punctuation+\" \")\n",
    "            \n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_infrequent_tokens(tokens,min_count=10):\n",
    "    word_count = {}\n",
    "    new_tokens = []\n",
    "    vocab = []\n",
    "    for token in tokens:\n",
    "        if token in word_count:\n",
    "            word_count[token] +=1\n",
    "        else:\n",
    "            word_count[token]=1\n",
    "    for token_word in tokens:\n",
    "        if word_count[token_word]>min_count:\n",
    "            new_tokens.append(token_word)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_latest_train(model_path):\n",
    "    checkpoints = os.listdir(model_path)\n",
    "    latest_checkpoint = \"\"\n",
    "    highest_epoch = 0\n",
    "    for checkpoint in checkpoints:\n",
    "        current_epoch = int(re.findall(\"weights-improvement-(\\d+)-(\\d+\\.\\d+).+\",checkpoint)[0][0])\n",
    "        if highest_epoch < current_epoch:\n",
    "            highest_epoch = current_epoch\n",
    "            latest_checkpoint = checkpoint\n",
    "    return latest_checkpoint,highest_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_scripts_cleaned = preprocess_text(all_scripts)\n",
    "tokens = all_scripts_cleaned.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cleaned tokens: 1300961\n"
     ]
    }
   ],
   "source": [
    "# print(len(tokens))\n",
    "cleaned_tokens = remove_infrequent_tokens(tokens)\n",
    "print(\"Number of cleaned tokens: {}\".format(len(cleaned_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocal length: 3567\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(cleaned_tokens))\n",
    "print(\"Vocal length: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 650470\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(vocab)\n",
    "characters2id = dict((c, i) for i, c in enumerate(vocab))\n",
    "id2characters = dict((i, c) for i, c in enumerate(vocab))\n",
    "section_length = 20\n",
    "step = 2\n",
    "sections = []\n",
    "section_labels = []\n",
    "for i in range(0,len(cleaned_tokens)-section_length-1,step):\n",
    "    section_in = cleaned_tokens[i:i+section_length]\n",
    "    section_out = cleaned_tokens[i+section_length]\n",
    "    sections.append([characters2id[word] for word in section_in])\n",
    "    section_labels.append(characters2id[section_out])\n",
    "\n",
    "print(\"Number of training examples: {}\".format(len(sections)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650470, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape(sections, (len(sections), section_length, 1))\n",
    "print(X.shape)\n",
    "X = X / float(vocab_length)\n",
    "y = np.zeros((len(sections),vocab_length))\n",
    "for i,section in enumerate(sections):\n",
    "    y[i,section_labels[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650470, 20, 1) (650470, 3567)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(1024, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(256))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/model3_friends/\"\n",
    "filepath=\"./model/model3_friends/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=False, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 19:28:17.089835 - Training resuming from epoch 100. Training for 10 epochs.\n",
      "Epoch 101/110\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 4.3110Epoch 00101: saving model to ./model/model3_friends/weights-improvement-101-4.3111.hdf5\n",
      "650470/650470 [==============================] - 529s 813us/step - loss: 4.3111\n",
      "Epoch 102/110\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 3.9853Epoch 00102: saving model to ./model/model3_friends/weights-improvement-102-3.9853.hdf5\n",
      "650470/650470 [==============================] - 530s 816us/step - loss: 3.9853\n",
      "Epoch 103/110\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 3.6681Epoch 00103: saving model to ./model/model3_friends/weights-improvement-103-3.6680.hdf5\n",
      "650470/650470 [==============================] - 533s 819us/step - loss: 3.6680\n",
      "Epoch 104/110\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 3.3685Epoch 00104: saving model to ./model/model3_friends/weights-improvement-104-3.3685.hdf5\n",
      "650470/650470 [==============================] - 533s 820us/step - loss: 3.3685\n",
      "Epoch 105/110\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 3.0971Epoch 00105: saving model to ./model/model3_friends/weights-improvement-105-3.0971.hdf5\n",
      "650470/650470 [==============================] - 543s 834us/step - loss: 3.0971\n",
      "Epoch 106/110\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 2.8585Epoch 00106: saving model to ./model/model3_friends/weights-improvement-106-2.8586.hdf5\n",
      "650470/650470 [==============================] - 534s 821us/step - loss: 2.8586\n",
      "Epoch 107/110\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 2.6521Epoch 00107: saving model to ./model/model3_friends/weights-improvement-107-2.6521.hdf5\n",
      "650470/650470 [==============================] - 534s 821us/step - loss: 2.6521\n",
      "Epoch 108/110\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 2.4753Epoch 00108: saving model to ./model/model3_friends/weights-improvement-108-2.4753.hdf5\n",
      "650470/650470 [==============================] - 537s 825us/step - loss: 2.4753\n",
      "Epoch 109/110\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 2.3221Epoch 00109: saving model to ./model/model3_friends/weights-improvement-109-2.3221.hdf5\n",
      "650470/650470 [==============================] - 537s 826us/step - loss: 2.3221\n",
      "Epoch 110/110\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 2.1912Epoch 00110: saving model to ./model/model3_friends/weights-improvement-110-2.1913.hdf5\n",
      "650470/650470 [==============================] - 538s 827us/step - loss: 2.1913\n",
      "2017-11-11 20:57:26.016873 - Training finished\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "num_epochs = 10\n",
    "checkpoint_filename,latest_epoch = get_latest_train(model_path)\n",
    "train_filepath = model_path+checkpoint_filename\n",
    "model.load_weights(train_filepath)\n",
    "print(\"{} - Training resuming from epoch {}. Training for {} epochs.\".format(datetime.now(),latest_epoch,num_epochs))\n",
    "model.fit(X, y, epochs=latest_epoch+num_epochs, batch_size=128, callbacks=callbacks_list,initial_epoch=latest_epoch)\n",
    "print(\"{} - Training finished\".format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing after 110 epochs. Filename: ./model/model3_friends/weights-improvement-110-2.1913.hdf5\n",
      "Seed: walked her home , and it was amazing how much we connected , y ' know ? then ah ,\n",
      "then ' passed my my , or , y , , ? , . \n",
      " monica : well , i ' ll call ! to to that . \n",
      " ross : oh , no , no , no , no , no , no , no , no , no ! no my be , we ' ! get ' , . , i , lets , right six . , i know you a , , youre ' know , but ' , a look . \n",
      " rachel : ( entering ) hi . \n",
      " rachel : hi . \n",
      " phoebe : ( couldn ooh i i \n",
      " oh know \n",
      " ross i i takes the . \n",
      " ross : ( - took ) ) , im . \n",
      " rachel : okay mon , . \n",
      " joey : oh , god on on , , it ive picture ! ! chandler rachel : ! \n",
      " monica : what pie \n",
      " \n",
      " chandler the a of ! ! rachel - ! more ! \n",
      " , its no ! sandwiches ! \n",
      " ! \n",
      " no : no , i , ! dont ! : okay , i got youre out waiting we , any , pass onto , : , really , dad , , over ! okay tape ( runs from ) and is gets . ) \n",
      " chandler : oh , god ! \n",
      " joey : what \n",
      " \n",
      " ! : did . . \n",
      " \n",
      " chandler : you , listen . \n",
      " joey : what , she , i ' m sorry ? \n",
      " joey : nothing , i ' m sorry i i ' m sorry , for ' s just that ' , pass . you , um - i know ' m crazy , , ' re take , , ' re don . \n",
      " ' i known chandler monica ! \n",
      " rachel : no know \n",
      " joey : yeah , i ' it pretty . \n",
      " joey : you ! ! , . think you go in and . . \n",
      " rachel : oh , god ! \n",
      " ross : ( . ! \n",
      " : : why , youve don ? ! ross - god ! \n",
      " ! : did ' s the , , ? i ' nothing never to job \n",
      " monica : made \n",
      " ross : okay , i , i ' m , go ' enough see , i . . , , i ' m had , had . . . i ' m you . \n",
      " chandler : oh . \n",
      " ross : huh . . . ( he . s i \n",
      " \n",
      " . : . ( ross in with and ) his . . ) \n",
      " monica : who , we ready with ? \n",
      " phoebe : oh , i , it , i i . . . just ' t to like . . i know got phoebe is ? i ' m gonna too\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "checkpoint_filename,latest_epoch = get_latest_train(model_path)\n",
    "test_filepath = model_path+checkpoint_filename\n",
    "print(\"Testing after {} epochs. Filename: {}\".format(latest_epoch,test_filepath))\n",
    "model.load_weights(test_filepath)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "start = np.random.randint(0, len(sections)-1)\n",
    "pattern = sections[start]\n",
    "print(\"Seed:\", \" \".join([id2characters[idx] for idx in pattern]).replace(\"NEWLINE\",\"\\n\"))\n",
    "predictions = []\n",
    "# generate characters\n",
    "for i in range(500):\n",
    "#     print([id2characters[idx] for idx in pattern])    \n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_length)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = id2characters[index]\n",
    "    seq_in = [id2characters[value] for value in pattern]\n",
    "    predictions.append(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\" \".join(predictions).replace(\"NEWLINE\",\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
