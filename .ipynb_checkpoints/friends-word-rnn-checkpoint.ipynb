{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./data/friends/all_scripts.txt\") as scripts_fileobj:\n",
    "    all_scripts = scripts_fileobj.read().strip().lower().decode('utf8').encode('ascii', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    dialogues = []\n",
    "    dialogue = []\n",
    "    for line in lines:\n",
    "        if \":\" in line:\n",
    "            dialogues.append(\" \".join(dialogue))\n",
    "            dialogue = []\n",
    "            dialogue.append(line)\n",
    "        else:\n",
    "            dialogue.append(line)\n",
    "    \n",
    "    text = \"\\n\".join(dialogues)\n",
    "    punctuations = set(re.findall(r\"[^a-zA-Z0-9 ]\",text))\n",
    "    for punctuation in punctuations:\n",
    "        if punctuation == \"\\n\":\n",
    "            text = text.replace(punctuation,\" NEWLINE \")\n",
    "        else:\n",
    "            text = text.replace(punctuation,\" \"+punctuation+\" \")\n",
    "            \n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_infrequent_tokens(tokens,min_count=10):\n",
    "    word_count = {}\n",
    "    new_tokens = []\n",
    "    vocab = []\n",
    "    for token in tokens:\n",
    "        if token in word_count:\n",
    "            word_count[token] +=1\n",
    "        else:\n",
    "            word_count[token]=1\n",
    "    for token_word in tokens:\n",
    "        if word_count[token_word]>min_count:\n",
    "            new_tokens.append(token_word)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_latest_train(model_path):\n",
    "    checkpoints = os.listdir(model_path)\n",
    "    latest_checkpoint = \"\"\n",
    "    highest_epoch = 0\n",
    "    for checkpoint in checkpoints:\n",
    "        current_epoch = int(re.findall(\"weights-improvement-(\\d+)-(\\d+\\.\\d+).+\",checkpoint)[0][0])\n",
    "        if highest_epoch < current_epoch:\n",
    "            highest_epoch = current_epoch\n",
    "            latest_checkpoint = checkpoint\n",
    "    return latest_checkpoint,highest_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_scripts_cleaned = preprocess_text(all_scripts)\n",
    "tokens = all_scripts_cleaned.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cleaned tokens: 1300961\n"
     ]
    }
   ],
   "source": [
    "# print(len(tokens))\n",
    "cleaned_tokens = remove_infrequent_tokens(tokens)\n",
    "print(\"Number of cleaned tokens: {}\".format(len(cleaned_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocal length: 3567\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(cleaned_tokens))\n",
    "print(\"Vocal length: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 650471\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(vocab)\n",
    "characters2id = dict((c, i) for i, c in enumerate(vocab))\n",
    "id2characters = dict((i, c) for i, c in enumerate(vocab))\n",
    "section_length = 20\n",
    "step = 2\n",
    "sections = []\n",
    "section_labels = []\n",
    "for i in range(0,len(cleaned_tokens)-section_length,step):\n",
    "    section_in = cleaned_tokens[i:i+section_length]\n",
    "    section_out = cleaned_tokens[i+section_length]\n",
    "    sections.append([characters2id[word] for word in section_in])\n",
    "    section_labels.append(characters2id[section_out])\n",
    "\n",
    "print(\"Number of training examples: {}\".format(len(sections)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650471, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape(sections, (len(sections), section_length, 1))\n",
    "print(X.shape)\n",
    "X = X / float(vocab_length)\n",
    "y = np.zeros((len(sections),vocab_length))\n",
    "for i,section in enumerate(sections):\n",
    "    y[i,section_labels[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650471, 20, 1) (650471, 3567)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(1024, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(256))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/model3_friends/\"\n",
    "filepath=model_path+\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 12:20:47.437004 - Training resuming from epoch 60. Training for 10 epochs.\n",
      "Epoch 61/70\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.5879Epoch 00061: loss improved from inf to 1.58793, saving model to ./model/model3_friends/weights-improvement-61-1.5879.hdf5\n",
      "650471/650471 [==============================] - 542s 834us/step - loss: 1.5879\n",
      "Epoch 62/70\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.5339Epoch 00062: loss improved from 1.58793 to 1.53397, saving model to ./model/model3_friends/weights-improvement-62-1.5340.hdf5\n",
      "650471/650471 [==============================] - 527s 810us/step - loss: 1.5340\n",
      "Epoch 63/70\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.5036Epoch 00063: loss improved from 1.53397 to 1.50367, saving model to ./model/model3_friends/weights-improvement-63-1.5037.hdf5\n",
      "650471/650471 [==============================] - 527s 810us/step - loss: 1.5037\n",
      "Epoch 64/70\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.4776Epoch 00064: loss improved from 1.50367 to 1.47761, saving model to ./model/model3_friends/weights-improvement-64-1.4776.hdf5\n",
      "650471/650471 [==============================] - 539s 828us/step - loss: 1.4776\n",
      "Epoch 65/70\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.4491Epoch 00065: loss improved from 1.47761 to 1.44909, saving model to ./model/model3_friends/weights-improvement-65-1.4491.hdf5\n",
      "650471/650471 [==============================] - 529s 813us/step - loss: 1.4491\n",
      "Epoch 66/70\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.4232Epoch 00066: loss improved from 1.44909 to 1.42322, saving model to ./model/model3_friends/weights-improvement-66-1.4232.hdf5\n",
      "650471/650471 [==============================] - 528s 812us/step - loss: 1.4232\n",
      "Epoch 67/70\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.3943Epoch 00067: loss improved from 1.42322 to 1.39436, saving model to ./model/model3_friends/weights-improvement-67-1.3944.hdf5\n",
      "650471/650471 [==============================] - 529s 813us/step - loss: 1.3944\n",
      "Epoch 68/70\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.3744Epoch 00068: loss improved from 1.39436 to 1.37444, saving model to ./model/model3_friends/weights-improvement-68-1.3744.hdf5\n",
      "650471/650471 [==============================] - 528s 812us/step - loss: 1.3744\n",
      "Epoch 69/70\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.3469Epoch 00069: loss improved from 1.37444 to 1.34691, saving model to ./model/model3_friends/weights-improvement-69-1.3469.hdf5\n",
      "650471/650471 [==============================] - 530s 815us/step - loss: 1.3469\n",
      "Epoch 70/70\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.3258Epoch 00070: loss improved from 1.34691 to 1.32582, saving model to ./model/model3_friends/weights-improvement-70-1.3258.hdf5\n",
      "650471/650471 [==============================] - 532s 818us/step - loss: 1.3258\n",
      "2017-11-11 13:49:19.704830 - Training finished\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "num_epochs = 10\n",
    "checkpoint_filename,latest_epoch = get_latest_train(model_path)\n",
    "train_filepath = model_path+checkpoint_filename\n",
    "model.load_weights(train_filepath)\n",
    "print(\"{} - Training resuming from epoch {}. Training for {} epochs.\".format(datetime.now(),latest_epoch,num_epochs))\n",
    "model.fit(X, y, epochs=latest_epoch+num_epochs, batch_size=128, callbacks=callbacks_list,initial_epoch=latest_epoch)\n",
    "print(\"{} - Training finished\".format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing after 70 epochs. Filename: ./model/model3_friends/weights-improvement-70-1.3258.hdf5\n",
      "Seed: : well , why don ' t we move this into the bedroom ? \n",
      " phoebe : really ? \n",
      "\n",
      "chandler : oh . \n",
      " do : yeah . \n",
      " rachel : i you know . \n",
      " phoebe : hey , . \n",
      " ross : yep ? which guess to ) \n",
      " ross : y you . \n",
      " rachel : oh . well in . . . we whole here whole to with that ' that attention . ] the everybody do . \n",
      " ross : yeah - yeah , ooh . i . ( to guy ) bye . \n",
      " the scene bye chandler ' s ross . ross ross phoebe the having . ] \n",
      " ross : ( , ) do s \n",
      " [ on , judy the ? \n",
      " \n",
      " : : , i is ' thought ! . i . ) the : ( know ) joey : the you . re for for your moving ! the was ? \n",
      " rachel : no , i really . \n",
      " joey : ( , ) and ? \n",
      " ross : oh , ive my im ! . \n",
      " rachel : oh . were , . was i ( m - i you rachel happy were and emma thinks i . . . \n",
      " ross : ( . ) \n",
      " : , i \n",
      " ross : i . ' . want you were so something . \n",
      " monica : no , we on a a ross . ( ross into ) which startled \n",
      " be : ? \n",
      " him : ( , other ) thank : hi said , one boss ! throw , , i i need ' \n",
      " better . love this to . , to chandler the phone ] . ] my at no , things so and , has , to come i your chandler and leave a . \n",
      " phoebe : ( , from . \n",
      " monica : no , you , i i want i . you richard , but . mean you an my . ( really and a get let uh the . ( rachel and at see look chandler , . he ) ) [ : wow , i \n",
      " ross . you : i if . \n",
      " chandler : oh ? \n",
      " phoebe : see , hey guys you , ohh . \n",
      " rachel : oh . \n",
      " phoebe : : , . you ' re gonna to to my chandler , with , no ' s the . \n",
      " chandler joey no , but ' ll ' it ! , late . pause goes gets a and as phoebe bathroom ' ) my a to because a joey : you me its , one i ' do tell you . my if i i ' be friend a now . gets d to chandler doing . . . ( is ) , no . i . ( . holds . \n",
      " . ( she ! the everyone hi ) monica : it , a have to to the . \n",
      " rachel : yeah . \n",
      " monica :\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "checkpoint_filename,latest_epoch = get_latest_train(model_path)\n",
    "test_filepath = model_path+checkpoint_filename\n",
    "print(\"Testing after {} epochs. Filename: {}\".format(latest_epoch,test_filepath))\n",
    "model.load_weights(test_filepath)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "start = np.random.randint(0, len(sections)-1)\n",
    "pattern = sections[start]\n",
    "print(\"Seed:\", \" \".join([id2characters[idx] for idx in pattern]).replace(\"NEWLINE\",\"\\n\"))\n",
    "predictions = []\n",
    "# generate characters\n",
    "for i in range(500):\n",
    "#     print([id2characters[idx] for idx in pattern])    \n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_length)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = id2characters[index]\n",
    "    seq_in = [id2characters[value] for value in pattern]\n",
    "    predictions.append(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\" \".join(predictions).replace(\"NEWLINE\",\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
