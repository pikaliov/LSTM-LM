{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./data/friends/all_scripts.txt\") as scripts_fileobj:\n",
    "    all_scripts = scripts_fileobj.read().strip().lower().decode('utf8').encode('ascii', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    dialogues = []\n",
    "    dialogue = []\n",
    "    for line in lines:\n",
    "        if \":\" in line:\n",
    "            dialogues.append(\" \".join(dialogue))\n",
    "            dialogue = []\n",
    "            dialogue.append(line)\n",
    "        else:\n",
    "            dialogue.append(line)\n",
    "    \n",
    "    text = \"\\n\".join(dialogues)\n",
    "    punctuations = set(re.findall(r\"[^a-zA-Z0-9 ]\",text))\n",
    "    for punctuation in punctuations:\n",
    "        if punctuation == \"\\n\":\n",
    "            text = text.replace(punctuation,\" NEWLINE \")\n",
    "        else:\n",
    "            text = text.replace(punctuation,\" \"+punctuation+\" \")\n",
    "            \n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_infrequent_tokens(tokens,min_count=10):\n",
    "    word_count = {}\n",
    "    new_tokens = []\n",
    "    vocab = []\n",
    "    for token in tokens:\n",
    "        if token in word_count:\n",
    "            word_count[token] +=1\n",
    "        else:\n",
    "            word_count[token]=1\n",
    "    for token_word in tokens:\n",
    "        if word_count[token_word]>min_count:\n",
    "            new_tokens.append(token_word)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_latest_train(model_path):\n",
    "    checkpoints = os.listdir(model_path)\n",
    "    latest_checkpoint = \"\"\n",
    "    highest_epoch = 0\n",
    "    for checkpoint in checkpoints:\n",
    "        current_epoch = int(re.findall(\"weights-improvement-(\\d+)-(\\d+\\.\\d+).+\",checkpoint)[0][0])\n",
    "        if highest_epoch < current_epoch:\n",
    "            highest_epoch = current_epoch\n",
    "            latest_checkpoint = checkpoint\n",
    "    return latest_checkpoint,highest_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_scripts_cleaned = preprocess_text(all_scripts)\n",
    "tokens = all_scripts_cleaned.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cleaned tokens: 1300961\n"
     ]
    }
   ],
   "source": [
    "# print(len(tokens))\n",
    "cleaned_tokens = remove_infrequent_tokens(tokens)\n",
    "print(\"Number of cleaned tokens: {}\".format(len(cleaned_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocal length: 3567\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(cleaned_tokens))\n",
    "print(\"Vocal length: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 650470\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(vocab)\n",
    "characters2id = dict((c, i) for i, c in enumerate(vocab))\n",
    "id2characters = dict((i, c) for i, c in enumerate(vocab))\n",
    "section_length = 20\n",
    "step = 2\n",
    "sections = []\n",
    "section_labels = []\n",
    "for i in range(0,len(cleaned_tokens)-section_length-1,step):\n",
    "    section_in = cleaned_tokens[i:i+section_length]\n",
    "    section_out = cleaned_tokens[i+section_length]\n",
    "    sections.append([characters2id[word] for word in section_in])\n",
    "    section_labels.append(characters2id[section_out])\n",
    "\n",
    "print(\"Number of training examples: {}\".format(len(sections)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650470, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape(sections, (len(sections), section_length, 1))\n",
    "print(X.shape)\n",
    "X = X / float(vocab_length)\n",
    "y = np.zeros((len(sections),vocab_length))\n",
    "for i,section in enumerate(sections):\n",
    "    y[i,section_labels[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650470, 20, 1) (650470, 3567)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(1024, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(256))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = \"./model/model3_friends/\"\n",
    "filepath=\"./model/model3_friends/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=False, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-12 02:07:41.127354 - Training resuming from epoch 110. Training for 40 epochs.\n",
      "Epoch 111/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 2.0893Epoch 00111: saving model to ./model/model3_friends/weights-improvement-111-2.0893.hdf5\n",
      "650470/650470 [==============================] - 587s 903us/step - loss: 2.0893\n",
      "Epoch 112/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.9792Epoch 00112: saving model to ./model/model3_friends/weights-improvement-112-1.9792.hdf5\n",
      "650470/650470 [==============================] - 529s 813us/step - loss: 1.9792\n",
      "Epoch 113/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.8856Epoch 00113: saving model to ./model/model3_friends/weights-improvement-113-1.8856.hdf5\n",
      "650470/650470 [==============================] - 529s 813us/step - loss: 1.8856\n",
      "Epoch 114/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.8003Epoch 00114: saving model to ./model/model3_friends/weights-improvement-114-1.8003.hdf5\n",
      "650470/650470 [==============================] - 529s 813us/step - loss: 1.8003\n",
      "Epoch 115/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.7252Epoch 00115: saving model to ./model/model3_friends/weights-improvement-115-1.7252.hdf5\n",
      "650470/650470 [==============================] - 529s 813us/step - loss: 1.7252\n",
      "Epoch 116/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.6562Epoch 00116: saving model to ./model/model3_friends/weights-improvement-116-1.6563.hdf5\n",
      "650470/650470 [==============================] - 528s 812us/step - loss: 1.6563\n",
      "Epoch 117/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.5929Epoch 00117: saving model to ./model/model3_friends/weights-improvement-117-1.5929.hdf5\n",
      "650470/650470 [==============================] - 528s 812us/step - loss: 1.5929\n",
      "Epoch 118/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.5318Epoch 00118: saving model to ./model/model3_friends/weights-improvement-118-1.5318.hdf5\n",
      "650470/650470 [==============================] - 533s 819us/step - loss: 1.5318\n",
      "Epoch 119/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.4798Epoch 00119: saving model to ./model/model3_friends/weights-improvement-119-1.4799.hdf5\n",
      "650470/650470 [==============================] - 533s 820us/step - loss: 1.4799\n",
      "Epoch 120/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.4286Epoch 00120: saving model to ./model/model3_friends/weights-improvement-120-1.4287.hdf5\n",
      "650470/650470 [==============================] - 534s 820us/step - loss: 1.4287\n",
      "Epoch 121/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.3798Epoch 00121: saving model to ./model/model3_friends/weights-improvement-121-1.3797.hdf5\n",
      "650470/650470 [==============================] - 534s 821us/step - loss: 1.3797\n",
      "Epoch 122/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.3397Epoch 00122: saving model to ./model/model3_friends/weights-improvement-122-1.3397.hdf5\n",
      "650470/650470 [==============================] - 535s 822us/step - loss: 1.3397\n",
      "Epoch 123/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.2985Epoch 00123: saving model to ./model/model3_friends/weights-improvement-123-1.2985.hdf5\n",
      "650470/650470 [==============================] - 535s 822us/step - loss: 1.2985\n",
      "Epoch 124/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.2624Epoch 00124: saving model to ./model/model3_friends/weights-improvement-124-1.2625.hdf5\n",
      "650470/650470 [==============================] - 534s 821us/step - loss: 1.2625\n",
      "Epoch 125/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.2249Epoch 00125: saving model to ./model/model3_friends/weights-improvement-125-1.2250.hdf5\n",
      "650470/650470 [==============================] - 534s 821us/step - loss: 1.2250\n",
      "Epoch 126/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.1921Epoch 00126: saving model to ./model/model3_friends/weights-improvement-126-1.1921.hdf5\n",
      "650470/650470 [==============================] - 534s 821us/step - loss: 1.1921\n",
      "Epoch 127/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.1619Epoch 00127: saving model to ./model/model3_friends/weights-improvement-127-1.1619.hdf5\n",
      "650470/650470 [==============================] - 534s 821us/step - loss: 1.1619\n",
      "Epoch 128/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.1314Epoch 00128: saving model to ./model/model3_friends/weights-improvement-128-1.1314.hdf5\n",
      "650470/650470 [==============================] - 534s 821us/step - loss: 1.1314\n",
      "Epoch 129/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.1031Epoch 00129: saving model to ./model/model3_friends/weights-improvement-129-1.1031.hdf5\n",
      "650470/650470 [==============================] - 534s 821us/step - loss: 1.1031\n",
      "Epoch 130/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.0792Epoch 00130: saving model to ./model/model3_friends/weights-improvement-130-1.0792.hdf5\n",
      "650470/650470 [==============================] - 534s 822us/step - loss: 1.0792\n",
      "Epoch 131/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.0533Epoch 00131: saving model to ./model/model3_friends/weights-improvement-131-1.0534.hdf5\n",
      "650470/650470 [==============================] - 534s 821us/step - loss: 1.0534\n",
      "Epoch 132/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.0277Epoch 00132: saving model to ./model/model3_friends/weights-improvement-132-1.0278.hdf5\n",
      "650470/650470 [==============================] - 534s 821us/step - loss: 1.0278\n",
      "Epoch 133/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 1.0070Epoch 00133: saving model to ./model/model3_friends/weights-improvement-133-1.0070.hdf5\n",
      "650470/650470 [==============================] - 534s 822us/step - loss: 1.0070\n",
      "Epoch 134/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.9856Epoch 00134: saving model to ./model/model3_friends/weights-improvement-134-0.9856.hdf5\n",
      "650470/650470 [==============================] - 534s 822us/step - loss: 0.9856\n",
      "Epoch 135/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.9665Epoch 00135: saving model to ./model/model3_friends/weights-improvement-135-0.9665.hdf5\n",
      "650470/650470 [==============================] - 534s 821us/step - loss: 0.9665\n",
      "Epoch 136/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.9501Epoch 00136: saving model to ./model/model3_friends/weights-improvement-136-0.9502.hdf5\n",
      "650470/650470 [==============================] - 534s 821us/step - loss: 0.9502\n",
      "Epoch 137/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.9301Epoch 00137: saving model to ./model/model3_friends/weights-improvement-137-0.9301.hdf5\n",
      "650470/650470 [==============================] - 534s 821us/step - loss: 0.9301\n",
      "Epoch 138/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.9129Epoch 00138: saving model to ./model/model3_friends/weights-improvement-138-0.9128.hdf5\n",
      "650470/650470 [==============================] - 536s 825us/step - loss: 0.9128\n",
      "Epoch 139/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.8954Epoch 00139: saving model to ./model/model3_friends/weights-improvement-139-0.8954.hdf5\n",
      "650470/650470 [==============================] - 536s 824us/step - loss: 0.8954\n",
      "Epoch 140/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.8801Epoch 00140: saving model to ./model/model3_friends/weights-improvement-140-0.8801.hdf5\n",
      "650470/650470 [==============================] - 536s 825us/step - loss: 0.8801\n",
      "Epoch 141/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.8670Epoch 00141: saving model to ./model/model3_friends/weights-improvement-141-0.8670.hdf5\n",
      "650470/650470 [==============================] - 534s 821us/step - loss: 0.8670\n",
      "Epoch 142/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.8484Epoch 00142: saving model to ./model/model3_friends/weights-improvement-142-0.8485.hdf5\n",
      "650470/650470 [==============================] - 533s 820us/step - loss: 0.8485\n",
      "Epoch 143/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.8370Epoch 00143: saving model to ./model/model3_friends/weights-improvement-143-0.8370.hdf5\n",
      "650470/650470 [==============================] - 523s 803us/step - loss: 0.8370\n",
      "Epoch 144/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.8243Epoch 00144: saving model to ./model/model3_friends/weights-improvement-144-0.8243.hdf5\n",
      "650470/650470 [==============================] - 523s 803us/step - loss: 0.8243\n",
      "Epoch 145/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.8107Epoch 00145: saving model to ./model/model3_friends/weights-improvement-145-0.8107.hdf5\n",
      "650470/650470 [==============================] - 523s 803us/step - loss: 0.8107\n",
      "Epoch 146/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.7985Epoch 00146: saving model to ./model/model3_friends/weights-improvement-146-0.7985.hdf5\n",
      "650470/650470 [==============================] - 522s 803us/step - loss: 0.7985\n",
      "Epoch 147/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.7866Epoch 00147: saving model to ./model/model3_friends/weights-improvement-147-0.7866.hdf5\n",
      "650470/650470 [==============================] - 524s 805us/step - loss: 0.7866\n",
      "Epoch 148/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.7746Epoch 00148: saving model to ./model/model3_friends/weights-improvement-148-0.7745.hdf5\n",
      "650470/650470 [==============================] - 527s 810us/step - loss: 0.7745\n",
      "Epoch 149/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.7664Epoch 00149: saving model to ./model/model3_friends/weights-improvement-149-0.7664.hdf5\n",
      "650470/650470 [==============================] - 523s 803us/step - loss: 0.7664\n",
      "Epoch 150/150\n",
      "650368/650470 [============================>.] - ETA: 0s - loss: 0.7558Epoch 00150: saving model to ./model/model3_friends/weights-improvement-150-0.7558.hdf5\n",
      "650470/650470 [==============================] - 637s 980us/step - loss: 0.7558\n",
      "2017-11-12 08:04:39.410122 - Training finished\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "num_epochs = 40\n",
    "checkpoint_filename,latest_epoch = get_latest_train(model_path)\n",
    "train_filepath = model_path+checkpoint_filename\n",
    "model.load_weights(train_filepath)\n",
    "print(\"{} - Training resuming from epoch {}. Training for {} epochs.\".format(datetime.now(),latest_epoch,num_epochs))\n",
    "model.fit(X, y, epochs=latest_epoch+num_epochs, batch_size=128, callbacks=callbacks_list,initial_epoch=latest_epoch)\n",
    "print(\"{} - Training finished\".format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing after 150 epochs. Filename: ./model/model3_friends/weights-improvement-150-0.7558.hdf5\n",
      "Seed: it every year ! that ' s why he ' s sending you to that play ! that ' s\n",
      "why the sent . there well can a hear , looks ! ' t have for ) right ' i your ' our about one , , , year up \n",
      " the else she we we a all know , ! , , kisses she . opens it again and ) face on the . ) \" rachel : now something , what ! ( joey starts fingers and ! made s to , who . ) \n",
      " rachel : well , her , that . havent picture a . . ( do up the up ) \n",
      " joey : ( , ) ) ) . \n",
      " [ scene : monica and rachel ' s , monica is ross the phone table chandler enters . ] \n",
      " chandler : hey . \n",
      " joey : hey . \n",
      " chandler : i just just did the to ? \n",
      " monica . it : what look you gonna about do ? \n",
      " monica : no , i ' m gonna to dr all the , long ? where is . \n",
      " ross : i ( ' be * \n",
      " rachel : i to know . \n",
      " ross : just i . . . i can at head someone . . . \n",
      " phoebe : ( . ( have what and tape joey ? . . good joey : family , . ' s gonna her me now he \n",
      " , for why : central perk that fun up : s phoebe phone is up of his . ] \n",
      " joey : ( on at ) ) ! , that youre : , i is and . . uh a so . what i have m still getting \n",
      " monica : because , i we actually look with . im . \n",
      " ross : oh . \n",
      " monica : well - , me the the walks a . , i have you go this each ! is ' , ? \n",
      " monica : i do you richard . \n",
      " chandler : so , . you , are ' s get , over why ' that wrong you be , there ' ve it \n",
      " know just right ' say in cat i t do t wait you meet i . you do t know my , you , to . i . ' ross is to ! wait : : stuff \n",
      " rachel ' s , , ' ' m all [ the in rest office right . ( , how the guys to around with with . \n",
      " \n",
      " ross : oh . i . . . . . . i . . . \n",
      " ross : look . \n",
      " rachel : ? . . ( . . . . ) ( : that just to into behind the ) what ross , what is to joey , where ? ) chandler , monica . ) \n",
      " : : yes . \n",
      " ? \n",
      " dont : you . \n",
      " ? : again bully\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "checkpoint_filename,latest_epoch = get_latest_train(model_path)\n",
    "test_filepath = model_path+checkpoint_filename\n",
    "print(\"Testing after {} epochs. Filename: {}\".format(latest_epoch,test_filepath))\n",
    "model.load_weights(test_filepath)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "start = np.random.randint(0, len(sections)-1)\n",
    "pattern = sections[start]\n",
    "print(\"Seed:\", \" \".join([id2characters[idx] for idx in pattern]).replace(\"NEWLINE\",\"\\n\"))\n",
    "predictions = []\n",
    "# generate characters\n",
    "for i in range(500):\n",
    "#     print([id2characters[idx] for idx in pattern])    \n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_length)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = id2characters[index]\n",
    "    seq_in = [id2characters[value] for value in pattern]\n",
    "    predictions.append(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\" \".join(predictions).replace(\"NEWLINE\",\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
