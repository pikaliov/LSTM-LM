{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./data/friends/all_scripts.txt\") as scripts_fileobj:\n",
    "    all_scripts = scripts_fileobj.read().strip().lower().decode('utf8').encode('ascii', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    dialogues = []\n",
    "    dialogue = []\n",
    "    for line in lines:\n",
    "        if \":\" in line:\n",
    "            dialogues.append(\" \".join(dialogue))\n",
    "            dialogue = []\n",
    "            dialogue.append(line)\n",
    "        else:\n",
    "            dialogue.append(line)\n",
    "    \n",
    "    text = \"\\n\".join(dialogues)\n",
    "    punctuations = set(re.findall(r\"[^a-zA-Z0-9 ]\",text))\n",
    "    for punctuation in punctuations:\n",
    "        if punctuation == \"\\n\":\n",
    "            text = text.replace(punctuation,\" NEWLINE \")\n",
    "        else:\n",
    "            text = text.replace(punctuation,\" \"+punctuation+\" \")\n",
    "            \n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_infrequent_tokens(tokens,min_count=10):\n",
    "    word_count = {}\n",
    "    new_tokens = []\n",
    "    vocab = []\n",
    "    for token in tokens:\n",
    "        if token in word_count:\n",
    "            word_count[token] +=1\n",
    "        else:\n",
    "            word_count[token]=1\n",
    "    for token_word in tokens:\n",
    "        if word_count[token_word]>min_count:\n",
    "            new_tokens.append(token_word)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_latest_train(model_path):\n",
    "    checkpoints = os.listdir(model_path)\n",
    "    latest_checkpoint = \"\"\n",
    "    highest_epoch = 0\n",
    "    for checkpoint in checkpoints:\n",
    "        current_epoch = int(re.findall(\"weights-improvement-(\\d+)-(\\d+\\.\\d+).+\",checkpoint)[0][0])\n",
    "        if highest_epoch < current_epoch:\n",
    "            highest_epoch = current_epoch\n",
    "            latest_checkpoint = checkpoint\n",
    "    return latest_checkpoint,highest_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_scripts_cleaned = preprocess_text(all_scripts)\n",
    "tokens = all_scripts_cleaned.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cleaned tokens: 1300961\n"
     ]
    }
   ],
   "source": [
    "# print(len(tokens))\n",
    "cleaned_tokens = remove_infrequent_tokens(tokens)\n",
    "print(\"Number of cleaned tokens: {}\".format(len(cleaned_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocal length: 3567\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(cleaned_tokens))\n",
    "print(\"Vocal length: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 650470\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(vocab)\n",
    "characters2id = dict((c, i) for i, c in enumerate(vocab))\n",
    "id2characters = dict((i, c) for i, c in enumerate(vocab))\n",
    "section_length = 20\n",
    "step = 2\n",
    "sections = []\n",
    "section_labels = []\n",
    "for i in range(1,len(cleaned_tokens)-section_length-1,step):\n",
    "    section_in = cleaned_tokens[i:i+section_length]\n",
    "    section_out = cleaned_tokens[i+section_length]\n",
    "    sections.append([characters2id[word] for word in section_in])\n",
    "    section_labels.append(characters2id[section_out])\n",
    "\n",
    "print(\"Number of training examples: {}\".format(len(sections)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650470, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape(sections, (len(sections), section_length, 1))\n",
    "print(X.shape)\n",
    "X = X / float(vocab_length)\n",
    "y = np.zeros((len(sections),vocab_length))\n",
    "for i,section in enumerate(sections):\n",
    "    y[i,section_labels[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650470, 20, 1) (650470, 3567)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(1024, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(256))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/model3_friends/\"\n",
    "filepath=model_path+\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=False, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "num_epochs = 10\n",
    "checkpoint_filename,latest_epoch = get_latest_train(model_path)\n",
    "train_filepath = model_path+checkpoint_filename\n",
    "model.load_weights(train_filepath)\n",
    "print(\"{} - Training resuming from epoch {}. Training for {} epochs.\".format(datetime.now(),latest_epoch,num_epochs))\n",
    "model.fit(X, y, epochs=latest_epoch+num_epochs, batch_size=128, callbacks=callbacks_list,initial_epoch=latest_epoch)\n",
    "print(\"{} - Training finished\".format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing after 90 epochs. Filename: ./model/model3_friends/weights-improvement-90-1.0168.hdf5\n",
      "Seed: idea that i want to talk to you ( points to chandler ) about , cause you work for a\n",
      "a company : : ! say for no ! ( they the have and covers ) kiss phoebe , and . ) . , thats - chapel to to , it ' ? and ? s find my , in , whole talk shes . . \n",
      " chandler : me dont . \n",
      " monica : okay , is it . \n",
      " phoebe : who monica a a the the ! ! yes : : , she is into office . \n",
      " \n",
      " rachel : i never i i ! chandler really that ! . , her is . . were ! ! ! youve you , ! his it , ! \n",
      " , : ( to a ) are what . \n",
      " joey : ( on ) the is ) . . . . ( . . . and . . . oh . ! phoebe : ( , ) ross with . \n",
      " chandler : great to okay some ? \n",
      " joey : yeah . \n",
      " chandler : you . you joey to i - me us . . joey m actually , joey just a is the , ! yeah . ( - havin um at like ) \n",
      " ross : ( , to guys ) , - there \n",
      " to monica . ? \n",
      " ross joey hi ) [ : ( entering ) yeah me im a . \n",
      " chandler : oh , i let m . . . im a a . the monica ' a want , so a some ! \n",
      " monica : all , it , . got chandler phoebe , to the you you this ? \n",
      " rachel : oh . \n",
      " \n",
      " : : yeah . \n",
      " joey : you ? . they we ' wanted happened ' the about with . and , absolutely re what came mr what . who monica of . ) \n",
      " rachel : oh . ! monica : hi dont ross : hey - i ' m still a . of ross this with , it one . . to a ross the . \n",
      " joey : ( , up , phone are . ) that , okay \n",
      " ! : : ( , in in , door and , , , me how how how got so , they . . . he . hits \n",
      " ross : i ) \n",
      " : : oh know \n",
      " . ( ( ross ) know . ) ( ( know the ) . she \n",
      " \n",
      " monica : may , what . \n",
      " scream : i smiling a you joey , ! \n",
      " rachel : what , here i the the . \n",
      " \n",
      " : : well , ) me hey ! ! ! : : you , you - it ? \n",
      " monica : i , you , i ' to ( this you , you a . \n",
      " chandler : yes , be , i ' ll , out back him . in , other\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "checkpoint_filename,latest_epoch = get_latest_train(model_path)\n",
    "test_filepath = model_path+checkpoint_filename\n",
    "print(\"Testing after {} epochs. Filename: {}\".format(latest_epoch,test_filepath))\n",
    "model.load_weights(test_filepath)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "start = np.random.randint(0, len(sections)-1)\n",
    "pattern = sections[start]\n",
    "print(\"Seed:\", \" \".join([id2characters[idx] for idx in pattern]).replace(\"NEWLINE\",\"\\n\"))\n",
    "predictions = []\n",
    "# generate characters\n",
    "for i in range(500):\n",
    "#     print([id2characters[idx] for idx in pattern])    \n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_length)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = id2characters[index]\n",
    "    seq_in = [id2characters[value] for value in pattern]\n",
    "    predictions.append(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\" \".join(predictions).replace(\"NEWLINE\",\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
