{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./data/friends/all_scripts.txt\") as scripts_fileobj:\n",
    "    all_scripts = scripts_fileobj.read().strip().lower().decode('utf8').encode('ascii', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    dialogues = []\n",
    "    dialogue = []\n",
    "    for line in lines:\n",
    "        if \":\" in line:\n",
    "            dialogues.append(\" \".join(dialogue))\n",
    "            dialogue = []\n",
    "            dialogue.append(line)\n",
    "        else:\n",
    "            dialogue.append(line)\n",
    "    \n",
    "    text = \"\\n\".join(dialogues)\n",
    "    punctuations = set(re.findall(r\"[^a-zA-Z0-9 ]\",text))\n",
    "    for punctuation in punctuations:\n",
    "        if punctuation == \"\\n\":\n",
    "            text = text.replace(punctuation,\" NEWLINE \")\n",
    "        else:\n",
    "            text = text.replace(punctuation,\" \"+punctuation+\" \")\n",
    "            \n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_infrequent_tokens(tokens,min_count=10):\n",
    "    word_count = {}\n",
    "    new_tokens = []\n",
    "    vocab = []\n",
    "    for token in tokens:\n",
    "        if token in word_count:\n",
    "            word_count[token] +=1\n",
    "        else:\n",
    "            word_count[token]=1\n",
    "    for token_word in tokens:\n",
    "        if word_count[token_word]>min_count:\n",
    "            new_tokens.append(token_word)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_latest_train(model_path):\n",
    "    checkpoints = os.listdir(model_path)\n",
    "    latest_checkpoint = \"\"\n",
    "    highest_epoch = 0\n",
    "    for checkpoint in checkpoints:\n",
    "        current_epoch = int(re.findall(\"weights-improvement-(\\d+)-(\\d+\\.\\d+).+\",checkpoint)[0][0])\n",
    "        if highest_epoch < current_epoch:\n",
    "            highest_epoch = current_epoch\n",
    "            latest_checkpoint = checkpoint\n",
    "    return latest_checkpoint,highest_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postprocess_text(predictions):\n",
    "    capitalise_words = [\"ross\",\"monica\",\"phoebe\",\"rachel\",\"chandler\",\"david\",\"joey\",\"i\"]\n",
    "    for i,prediction in enumerate(predictions):\n",
    "        if prediction in  capitalise_words:\n",
    "            predictions[i]=prediction.title()\n",
    "    text = \" \".join(predictions)\n",
    "    text = text.replace(\"NEWLINE\",\"\\n\")\n",
    "    text = text.replace(\" .\",\".\")\n",
    "    text = text.replace(\" ,\",\",\")\n",
    "    text = text.replace(\" :\",\":\")\n",
    "    text = text.replace(\" ' \",\"'\")\n",
    "    text = text.replace(\" ( \",\"(\")\n",
    "    text = text.replace(\" ) \",\")\")\n",
    "    text = text.replace(\" !\",\"!\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_scripts_cleaned = preprocess_text(all_scripts)\n",
    "tokens = all_scripts_cleaned.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cleaned tokens: 1300961\n"
     ]
    }
   ],
   "source": [
    "# print(len(tokens))\n",
    "cleaned_tokens = remove_infrequent_tokens(tokens)\n",
    "print(\"Number of cleaned tokens: {}\".format(len(cleaned_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocal length: 3567\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(cleaned_tokens))\n",
    "print(\"Vocal length: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 650470\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(vocab)\n",
    "characters2id = dict((c, i) for i, c in enumerate(vocab))\n",
    "id2characters = dict((i, c) for i, c in enumerate(vocab))\n",
    "section_length = 20\n",
    "step = 2\n",
    "sections = []\n",
    "section_labels = []\n",
    "for i in range(0,len(cleaned_tokens)-section_length-1,step):\n",
    "    section_in = cleaned_tokens[i:i+section_length]\n",
    "    section_out = cleaned_tokens[i+section_length]\n",
    "    sections.append([characters2id[word] for word in section_in])\n",
    "    section_labels.append(characters2id[section_out])\n",
    "\n",
    "print(\"Number of training examples: {}\".format(len(sections)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650470, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape(sections, (len(sections), section_length, 1))\n",
    "print(X.shape)\n",
    "X = X / float(vocab_length)\n",
    "y = np.zeros((len(sections),vocab_length))\n",
    "for i,section in enumerate(sections):\n",
    "    y[i,section_labels[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650470, 20, 1) (650470, 3567)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(1024, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(256))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = \"./model/model_friends/\"\n",
    "filepath=\"./model/model_friends/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=False, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "num_epochs = 40\n",
    "checkpoint_filename,latest_epoch = get_latest_train(model_path)\n",
    "train_filepath = model_path+checkpoint_filename\n",
    "model.load_weights(train_filepath)\n",
    "print(\"{} - Training resuming from epoch {}. Training for {} epochs.\".format(datetime.now(),latest_epoch,num_epochs))\n",
    "model.fit(X, y, epochs=latest_epoch+num_epochs, batch_size=128, callbacks=callbacks_list,initial_epoch=latest_epoch)\n",
    "print(\"{} - Training finished\".format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing after 73 epochs. Filename: ./model/model_friends/weights-improvement-73-1.0198.hdf5\n",
      "Seed: , go ! ( the paleontologists starts chasing them ) \n",
      " [ scene : game room , monica and mike\n",
      "are still playing ping pong ](Chandler and Phoebe look bored to death. Monica scores and laughs)\n",
      " Phoebe: ok, I'm see to Joey to someone you to, I'm it'you a with a, or.'' s go the whole of be, to, me close at a ?'' t. that! ?(of over of)happy)one, I never them he it read in through.. \n",
      " Phoebe::, what, nothing, too, I should see just a the means. are Monica. know, and, no ever and a are than so that: I get this.. Monica, what to at him do and and'to a. \n",
      " Rachel: well, I gotta a the last the drake... \n",
      " Joey: (... what. everybody about was the window comes about with ] ] [ business. ] \n",
      " Joey: hey, cool, hold this ? \n",
      " Joey: -... had in I. \n",
      " Rachel: I! it,. cant'\n",
      " sound the in the youre, but I nothing to want. mean do a the it ha of the. \n",
      " Joey Phoebe what: what, I'm ? \n",
      " Phoebe: you my god. \n",
      " Ross: I,, I'm going uh this t. \n",
      " Joey: what,, only you we are. I a over. all the them just, friend umm later. im can, by, uhm ? \n",
      " Monica: green right, well have m giving go again. \n",
      " Chandler: and, so'' you tell that. \n",
      " Monica:(?.. ? we't know. we I what Chandler about this with much...(Joey enters)Monica the.. of a Joey: starts you you,, - she get have, dance.. \n",
      " \n",
      ": oh, I think you... I \n",
      " mrs:. \n",
      " Phoebe: oh, you, like know don you married \n",
      " really a. why Ross Monica, a take. you get you, who has a side. there a and season of kisses, you you Chandler. \n",
      " Chandler: oh, im, I didnt, get. \n",
      " Joey: and uh stay it ?, pulls.. \n",
      " Rachel: yes. is. okay,. was: yeah, to know you. \n",
      " Ross: no,. \n",
      " Chandler:.. \n",
      " Rachel: hi I \n",
      " Ross:(what phone dressed)hey, god I\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "checkpoint_filename,latest_epoch = get_latest_train(model_path)\n",
    "test_filepath = model_path+checkpoint_filename\n",
    "print(\"Testing after {} epochs. Filename: {}\".format(latest_epoch,test_filepath))\n",
    "model.load_weights(test_filepath)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "start = np.random.randint(0, len(sections)-1)\n",
    "pattern = sections[start]\n",
    "print(\"Seed:\", \" \".join([id2characters[idx] for idx in pattern]).replace(\"NEWLINE\",\"\\n\"))\n",
    "predictions = []\n",
    "# generate characters\n",
    "for i in range(500):\n",
    "#     print([id2characters[idx] for idx in pattern])    \n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_length)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = id2characters[index]\n",
    "    seq_in = [id2characters[value] for value in pattern]\n",
    "    predictions.append(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(postprocess_text(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
