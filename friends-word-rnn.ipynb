{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./data/friends/all_scripts.txt\") as scripts_fileobj:\n",
    "    all_scripts = scripts_fileobj.read().strip().lower().decode('utf8').encode('ascii', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    dialogues = []\n",
    "    dialogue = []\n",
    "    for line in lines:\n",
    "        if \":\" in line:\n",
    "            dialogues.append(\" \".join(dialogue))\n",
    "            dialogue = []\n",
    "            dialogue.append(line)\n",
    "        else:\n",
    "            dialogue.append(line)\n",
    "    \n",
    "    text = \"\\n\".join(dialogues)\n",
    "    punctuations = set(re.findall(r\"[^a-zA-Z0-9 ]\",text))\n",
    "    for punctuation in punctuations:\n",
    "        if punctuation == \"\\n\":\n",
    "            text = text.replace(punctuation,\" NEWLINE \")\n",
    "        else:\n",
    "            text = text.replace(punctuation,\" \"+punctuation+\" \")\n",
    "            \n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_infrequent_tokens(tokens,min_count=10):\n",
    "    word_count = {}\n",
    "    new_tokens = []\n",
    "    vocab = []\n",
    "    for token in tokens:\n",
    "        if token in word_count:\n",
    "            word_count[token] +=1\n",
    "        else:\n",
    "            word_count[token]=1\n",
    "    for token_word in tokens:\n",
    "        if word_count[token_word]>min_count:\n",
    "            new_tokens.append(token_word)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_scripts_cleaned = preprocess_text(all_scripts)\n",
    "tokens = all_scripts_cleaned.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cleaned tokens: 1300961\n"
     ]
    }
   ],
   "source": [
    "# print(len(tokens))\n",
    "cleaned_tokens = remove_infrequent_tokens(tokens)\n",
    "print(\"Number of cleaned tokens: {}\".format(len(cleaned_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocal length: 3567\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(cleaned_tokens))\n",
    "print(\"Vocal length: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 650471\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(vocab)\n",
    "characters2id = dict((c, i) for i, c in enumerate(vocab))\n",
    "id2characters = dict((i, c) for i, c in enumerate(vocab))\n",
    "section_length = 20\n",
    "step = 2\n",
    "sections = []\n",
    "section_labels = []\n",
    "for i in range(0,len(cleaned_tokens)-section_length,step):\n",
    "    section_in = cleaned_tokens[i:i+section_length]\n",
    "    section_out = cleaned_tokens[i+section_length]\n",
    "    sections.append([characters2id[word] for word in section_in])\n",
    "    section_labels.append(characters2id[section_out])\n",
    "\n",
    "print(\"Number of training examples: {}\".format(len(sections)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650471, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape(sections, (len(sections), section_length, 1))\n",
    "print(X.shape)\n",
    "X = X / float(vocab_length)\n",
    "y = np.zeros((len(sections),vocab_length))\n",
    "for i,section in enumerate(sections):\n",
    "    y[i,section_labels[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650471, 20, 1) (650471, 3567)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(1024, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(256))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"./model/model3_friends/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 5.2049Epoch 00021: loss improved from inf to 5.20487, saving model to ./model/model3_friends/weights-improvement-21-5.2049.hdf5\n",
      "650471/650471 [==============================] - 542s 834us/step - loss: 5.2049\n",
      "Epoch 22/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 4.8551Epoch 00022: loss improved from 5.20487 to 4.85511, saving model to ./model/model3_friends/weights-improvement-22-4.8551.hdf5\n",
      "650471/650471 [==============================] - 529s 814us/step - loss: 4.8551\n",
      "Epoch 23/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 4.7099Epoch 00023: loss improved from 4.85511 to 4.70991, saving model to ./model/model3_friends/weights-improvement-23-4.7099.hdf5\n",
      "650471/650471 [==============================] - 531s 816us/step - loss: 4.7099\n",
      "Epoch 24/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 4.5989Epoch 00024: loss improved from 4.70991 to 4.59893, saving model to ./model/model3_friends/weights-improvement-24-4.5989.hdf5\n",
      "650471/650471 [==============================] - 531s 816us/step - loss: 4.5989\n",
      "Epoch 25/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 4.4947Epoch 00025: loss improved from 4.59893 to 4.49470, saving model to ./model/model3_friends/weights-improvement-25-4.4947.hdf5\n",
      "650471/650471 [==============================] - 530s 815us/step - loss: 4.4947\n",
      "Epoch 26/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 4.3859Epoch 00026: loss improved from 4.49470 to 4.38587, saving model to ./model/model3_friends/weights-improvement-26-4.3859.hdf5\n",
      "650471/650471 [==============================] - 530s 814us/step - loss: 4.3859\n",
      "Epoch 27/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 4.2695Epoch 00027: loss improved from 4.38587 to 4.26957, saving model to ./model/model3_friends/weights-improvement-27-4.2696.hdf5\n",
      "650471/650471 [==============================] - 534s 820us/step - loss: 4.2696\n",
      "Epoch 28/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 4.1461Epoch 00028: loss improved from 4.26957 to 4.14618, saving model to ./model/model3_friends/weights-improvement-28-4.1462.hdf5\n",
      "650471/650471 [==============================] - 534s 821us/step - loss: 4.1462\n",
      "Epoch 29/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 4.0136Epoch 00029: loss improved from 4.14618 to 4.01364, saving model to ./model/model3_friends/weights-improvement-29-4.0136.hdf5\n",
      "650471/650471 [==============================] - 535s 823us/step - loss: 4.0136\n",
      "Epoch 30/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 3.8782Epoch 00030: loss improved from 4.01364 to 3.87823, saving model to ./model/model3_friends/weights-improvement-30-3.8782.hdf5\n",
      "650471/650471 [==============================] - 535s 823us/step - loss: 3.8782\n",
      "Epoch 31/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 3.7404Epoch 00031: loss improved from 3.87823 to 3.74042, saving model to ./model/model3_friends/weights-improvement-31-3.7404.hdf5\n",
      "650471/650471 [==============================] - 537s 825us/step - loss: 3.7404\n",
      "Epoch 32/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 3.6043Epoch 00032: loss improved from 3.74042 to 3.60429, saving model to ./model/model3_friends/weights-improvement-32-3.6043.hdf5\n",
      "650471/650471 [==============================] - 538s 828us/step - loss: 3.6043\n",
      "Epoch 33/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 3.4724Epoch 00033: loss improved from 3.60429 to 3.47235, saving model to ./model/model3_friends/weights-improvement-33-3.4724.hdf5\n",
      "650471/650471 [==============================] - 537s 825us/step - loss: 3.4724\n",
      "Epoch 34/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 3.3443Epoch 00034: loss improved from 3.47235 to 3.34427, saving model to ./model/model3_friends/weights-improvement-34-3.3443.hdf5\n",
      "650471/650471 [==============================] - 540s 830us/step - loss: 3.3443\n",
      "Epoch 35/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 3.2239Epoch 00035: loss improved from 3.34427 to 3.22396, saving model to ./model/model3_friends/weights-improvement-35-3.2240.hdf5\n",
      "650471/650471 [==============================] - 539s 829us/step - loss: 3.2240\n",
      "Epoch 36/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 3.1083Epoch 00036: loss improved from 3.22396 to 3.10827, saving model to ./model/model3_friends/weights-improvement-36-3.1083.hdf5\n",
      "650471/650471 [==============================] - 536s 823us/step - loss: 3.1083\n",
      "Epoch 37/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 3.0010Epoch 00037: loss improved from 3.10827 to 3.00094, saving model to ./model/model3_friends/weights-improvement-37-3.0009.hdf5\n",
      "650471/650471 [==============================] - 535s 823us/step - loss: 3.0009\n",
      "Epoch 38/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 2.8969Epoch 00038: loss improved from 3.00094 to 2.89688, saving model to ./model/model3_friends/weights-improvement-38-2.8969.hdf5\n",
      "650471/650471 [==============================] - 536s 824us/step - loss: 2.8969\n",
      "Epoch 39/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 2.7998Epoch 00039: loss improved from 2.89688 to 2.79982, saving model to ./model/model3_friends/weights-improvement-39-2.7998.hdf5\n",
      "650471/650471 [==============================] - 536s 824us/step - loss: 2.7998\n",
      "Epoch 40/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 2.7064Epoch 00040: loss improved from 2.79982 to 2.70642, saving model to ./model/model3_friends/weights-improvement-40-2.7064.hdf5\n",
      "650471/650471 [==============================] - 536s 824us/step - loss: 2.7064\n",
      "Epoch 41/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 2.6194Epoch 00041: loss improved from 2.70642 to 2.61938, saving model to ./model/model3_friends/weights-improvement-41-2.6194.hdf5\n",
      "650471/650471 [==============================] - 536s 823us/step - loss: 2.6194\n",
      "Epoch 42/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 2.5378Epoch 00042: loss improved from 2.61938 to 2.53776, saving model to ./model/model3_friends/weights-improvement-42-2.5378.hdf5\n",
      "650471/650471 [==============================] - 535s 823us/step - loss: 2.5378\n",
      "Epoch 43/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 2.4558Epoch 00043: loss improved from 2.53776 to 2.45585, saving model to ./model/model3_friends/weights-improvement-43-2.4558.hdf5\n",
      "650471/650471 [==============================] - 536s 824us/step - loss: 2.4558\n",
      "Epoch 44/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 2.3838Epoch 00044: loss improved from 2.45585 to 2.38379, saving model to ./model/model3_friends/weights-improvement-44-2.3838.hdf5\n",
      "650471/650471 [==============================] - 535s 823us/step - loss: 2.3838\n",
      "Epoch 45/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 2.3142Epoch 00045: loss improved from 2.38379 to 2.31426, saving model to ./model/model3_friends/weights-improvement-45-2.3143.hdf5\n",
      "650471/650471 [==============================] - 536s 824us/step - loss: 2.3143\n",
      "Epoch 46/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 2.2462Epoch 00046: loss improved from 2.31426 to 2.24623, saving model to ./model/model3_friends/weights-improvement-46-2.2462.hdf5\n",
      "650471/650471 [==============================] - 536s 824us/step - loss: 2.2462\n",
      "Epoch 47/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 2.1837Epoch 00047: loss improved from 2.24623 to 2.18378, saving model to ./model/model3_friends/weights-improvement-47-2.1838.hdf5\n",
      "650471/650471 [==============================] - 536s 825us/step - loss: 2.1838\n",
      "Epoch 48/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 2.1224- ETA: 1Epoch 00048: loss improved from 2.18378 to 2.12240, saving model to ./model/model3_friends/weights-improvement-48-2.1224.hdf5\n",
      "650471/650471 [==============================] - 537s 825us/step - loss: 2.1224\n",
      "Epoch 49/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650368/650471 [============================>.] - ETA: 0s - loss: 2.0642Epoch 00049: loss improved from 2.12240 to 2.06421, saving model to ./model/model3_friends/weights-improvement-49-2.0642.hdf5\n",
      "650471/650471 [==============================] - 530s 814us/step - loss: 2.0642\n",
      "Epoch 50/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 2.0128Epoch 00050: loss improved from 2.06421 to 2.01277, saving model to ./model/model3_friends/weights-improvement-50-2.0128.hdf5\n",
      "650471/650471 [==============================] - 528s 812us/step - loss: 2.0128\n",
      "Epoch 51/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.9601Epoch 00051: loss improved from 2.01277 to 1.96012, saving model to ./model/model3_friends/weights-improvement-51-1.9601.hdf5\n",
      "650471/650471 [==============================] - 528s 812us/step - loss: 1.9601\n",
      "Epoch 52/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.9124Epoch 00052: loss improved from 1.96012 to 1.91240, saving model to ./model/model3_friends/weights-improvement-52-1.9124.hdf5\n",
      "650471/650471 [==============================] - 528s 812us/step - loss: 1.9124\n",
      "Epoch 53/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.8661Epoch 00053: loss improved from 1.91240 to 1.86611, saving model to ./model/model3_friends/weights-improvement-53-1.8661.hdf5\n",
      "650471/650471 [==============================] - 529s 813us/step - loss: 1.8661\n",
      "Epoch 54/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.8184Epoch 00054: loss improved from 1.86611 to 1.81850, saving model to ./model/model3_friends/weights-improvement-54-1.8185.hdf5\n",
      "650471/650471 [==============================] - 528s 812us/step - loss: 1.8185\n",
      "Epoch 55/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.7745Epoch 00055: loss improved from 1.81850 to 1.77450, saving model to ./model/model3_friends/weights-improvement-55-1.7745.hdf5\n",
      "650471/650471 [==============================] - 529s 813us/step - loss: 1.7745\n",
      "Epoch 56/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.7370Epoch 00056: loss improved from 1.77450 to 1.73706, saving model to ./model/model3_friends/weights-improvement-56-1.7371.hdf5\n",
      "650471/650471 [==============================] - 529s 813us/step - loss: 1.7371\n",
      "Epoch 57/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.6984Epoch 00057: loss improved from 1.73706 to 1.69842, saving model to ./model/model3_friends/weights-improvement-57-1.6984.hdf5\n",
      "650471/650471 [==============================] - 529s 813us/step - loss: 1.6984\n",
      "Epoch 58/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.6619Epoch 00058: loss improved from 1.69842 to 1.66186, saving model to ./model/model3_friends/weights-improvement-58-1.6619.hdf5\n",
      "650471/650471 [==============================] - 528s 812us/step - loss: 1.6619\n",
      "Epoch 59/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.6280Epoch 00059: loss improved from 1.66186 to 1.62807, saving model to ./model/model3_friends/weights-improvement-59-1.6281.hdf5\n",
      "650471/650471 [==============================] - 528s 811us/step - loss: 1.6281\n",
      "Epoch 60/60\n",
      "650368/650471 [============================>.] - ETA: 0s - loss: 1.5949- ETEpoch 00060: loss improved from 1.62807 to 1.59492, saving model to ./model/model3_friends/weights-improvement-60-1.5949.hdf5\n",
      "650471/650471 [==============================] - 528s 812us/step - loss: 1.5949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1d224ce5d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=60, batch_size=128, callbacks=callbacks_list,initial_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: will be fun ! ohh ! yay ! oh ! okay , ooh , lets plan the wedding reception .\n",
      "\n",
      " rachel : oh , i , i wouldn think . s up for , . \n",
      " monica : yeah , he think have you what they of \n",
      " party \n",
      " me : no , you have re right . i . don i hate i \n",
      " you . i , who would everyone , you , i i up gotta way and . \n",
      " mr : you , . \n",
      " phoebe : you , but was , . \n",
      " monica : ( , yeah . . ' ( t the her her her she \n",
      " , : t like \n",
      " chandler : alright , no . \n",
      " ross : oh , - . \n",
      " joey : oh , bye . \n",
      " ross : hey , no , ' , ok , be , ross , don ' t be why \n",
      " rachel at hands . \n",
      " gavin : i , i ' m ' . we ' \n",
      " look . and on bathroom is they i . . \n",
      " phoebe : ( , you ) ) ) \n",
      " ! \n",
      " ( , start the over joey the . . ) tries , : oh , this , you really that up like \n",
      " rachel : yeah , i , i , i know ll . i removes as so ' that ive mine i , i have to i , and that with would now . \n",
      " rachel : oh ? \n",
      " ross : no , you , ' you little to your ? \n",
      " rachel : oh , i ' m sorry . . . \n",
      " ross : y i . , ' . \n",
      " joey : ok , i . . . ' , i . . . she ' m mirror . ) \n",
      " joey : yeah , yeah , ' . you ? ? - into lot it . . \n",
      " chandler : no , i ' ' got you . . ross monica ' ' s , new . she m sorry all \n",
      " chandler ) \n",
      " chandler : oh , good . did ' s gunther , that off , , do like , . ' s a . is ' each gonna you me me , ' s sick \n",
      " monica : ( , ) rachel ? they ? [ , ! to . \n",
      " . : . can chandler ross later guy and and . \n",
      " chandler : oh yeah ! \n",
      " joey : ( he ) joey : what my bye . s go s one ? \n",
      " chandler : oh . \n",
      " joey : no . \n",
      " chandler : what would look . \n",
      " joey : yes , yeah . \n",
      " chandler : oh , how ! \n",
      " are : yeah ! i he not a to ! under jacket this you over and ! . \n",
      " monica . \n",
      " : : no , . ( that , all . ) , . \n",
      " : no my so ,\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_filename = \"./model/model3_friends/weights-improvement-60-1.5949.hdf5\"\n",
    "model.load_weights(test_filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "start = np.random.randint(0, len(sections)-1)\n",
    "pattern = sections[start]\n",
    "print(\"Seed:\", \" \".join([id2characters[idx] for idx in pattern]).replace(\"NEWLINE\",\"\\n\"))\n",
    "predictions = []\n",
    "# generate characters\n",
    "for i in range(500):\n",
    "#     print([id2characters[idx] for idx in pattern])    \n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_length)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = id2characters[index]\n",
    "    seq_in = [id2characters[value] for value in pattern]\n",
    "    predictions.append(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\" \".join(predictions).replace(\"NEWLINE\",\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
